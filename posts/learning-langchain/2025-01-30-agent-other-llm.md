---
title: "Agent ì£¼ìš”ê¸°ëŠ¥ | ë‹¤ë¥¸ LLM í˜¸ì¶œí•´ë³´ê¸°"
date: 2025-01-30
---

## OpenAI ì™¸ì˜ ë„êµ¬ í˜¸ì¶œ Agent ë§Œë“¤ê¸°(Tool Calling Agent with others)

OpenAI ì™¸ì—ë„ Langchainì€ `Anthropic`, `Google Gemini`, `Mistral`, `Together.ai`, `Ollama`ê³¼ ê°™ì€ ì—¬ëŸ¬ ê³µê¸‰ì êµ¬í˜„ì„ ì§€ì›í•œë‹¤.

ì—¬ê¸°ì„œëŠ” ë‹¤ì–‘í•œ LLMìœ¼ë¡œ ë„êµ¬ í˜¸ì¶œ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ì‚´í´ë³´ì.

Source : https://python.langchain.com/v0.1/docs/modules/agents/agent_types/tool_calling/

```python
from dotenv import load_dotenv

load_dotenv()

from langchain_teddynote import logging
logging.langsmith("agent-other-llm")
```

    LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.
    [í”„ë¡œì íŠ¸ëª…]
    agent-other-llm

```python
from langchain.tools import tool
from typing import List, Dict
from langchain_teddynote.tools import GoogleNews

# Define tool
@tool
def search_google_news(query: str)-> List[Dict[str, str]] :
    """Search google news by input keyword"""
    news_tool = GoogleNews()
    return news_tool.search_by_keyword(query, k=5)


print(f"ë„êµ¬ ì´ë¦„ : {search_google_news.name}")
print(f"ë„êµ¬ ì„¤ëª… : {search_google_news.description}")

```

    ë„êµ¬ ì´ë¦„ : search_google_news
    ë„êµ¬ ì„¤ëª… : Search google news by input keyword

```python
# Define tool
tools = [search_google_news]
```

### Agent ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±

- `chat_history` : ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì €ì¥í•˜ëŠ” ë³€ìˆ˜ (ë©€í‹°í„´ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´, ìƒëµ ê°€ëŠ¥í•©ë‹ˆë‹¤.)
- `agent_scratchpad` : ì—ì´ì „íŠ¸ê°€ ì„ì‹œë¡œ ì €ì¥í•˜ëŠ” ë³€ìˆ˜
- `input` : ì‚¬ìš©ìì˜ ì…ë ¥

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain.agents import create_tool_calling_agent

# create prompt
# í”„ë¡¬í”„íŠ¸ëŠ” ì—ì´ì „íŠ¸ì—ê²Œ ëª¨ë¸ì´ ìˆ˜í–‰í•  ì‘ì—…ì„ ì„¤ëª…í•˜ëŠ” í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•œë‹¤ (ë„êµ¬ì˜ ì´ë¦„ê³¼ ì—­í• ì„ ì…ë ¥)
prompt = ChatPromptTemplate.from_messages([
    (
        "system",
        "You are a helpful assistant that can search google news by input keyword"
     ),
     ("placeholder", "{chat_history}"),
     ("human", "{input}"),
     ("placeholder", "{agent_scratchpad}"),
])
```

### Tool Calling ì„ ì§€ì›í•˜ëŠ” LLM List

ì‹¤ìŠµ ì§„í–‰ì„ ìœ„í•´ ì•„ë˜ ë‚´ìš©ì„ ì„¤ì •í•œë‹¤.

- Anthropic
  - [Anthropic API í‚¤ ë°œê¸‰ ê´€ë ¨](https://console.anthropic.com/settings/keys)
  - .env íŒŒì¼ ë‚´ ANTHROPIC_API_KEY ì— ë°œê¸‰ë°›ì€ í‚¤ ì„¤ì •
- Gemini
  - [Gemini API í‚¤ ë°œê¸‰ ê´€ë ¨](https://aistudio.google.com/app/apikey?hl=ko)
  - .env íŒŒì¼ ë‚´ GOOGLE_API_KEY ì— ë°œê¸‰ë°›ì€ í‚¤ ì„¤ì •
- Together AI
  - [Together AI API í‚¤ ë°œê¸‰ ê´€ë ¨](https://api.together.ai/signin)
  - .env íŒŒì¼ ë‚´ TOGETHER_API_KEY ì— ë°œê¸‰ë°›ì€ í‚¤ ì„¤ì •
- Ollama
  - [Ollama Tool Calling ì§€ì› ëª¨ë¸ ë¦¬ìŠ¤íŠ¸](https://ollama.com/search?c=tools)
  - [ì´ë²ˆ ì‹¤ìŠµì— ì‚¬ìš©í•  llama3.1 ëª¨ë¸](https://ollama.com/library/llama3.1)
  - í„°ë¯¸ë„ ì°½ì— ollama pull llama3.1 ëª…ë ¹ì–´ë¥¼ ì…ë ¥í•˜ì—¬ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
  - ì´ì „ì— Ollama ë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ë‹¤ë©´, [Ollama](https://wikidocs.net/233805) ë¥¼ ì°¸ê³ í•œë‹¤.

langchain-ollama ì„¤ì¹˜ë¥¼ í•œ ë’¤ ì§„í–‰í•œë‹¤.

```python

# ! pip install -qU langchain-ollama==0.1.3
# ! pip install -U notebook
```

```python
# ! pip install -q langchain_anthropic langchain_google_vertexai langchain_ollama
```

```python
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI
# from langchain_google_vertexai import ChatVertexAI
from langchain_openai import ChatOpenAI
from langchain_ollama import ChatOllama
import os

# GPT-4o-mini
gpt = ChatOpenAI(model="gpt-4o-mini")

# Claude 3.5 Sonnet
claude = ChatAnthropic(model="claude-3-5-sonnet-20240620", temperature=0)

# Gemini 1.5 Pro
gemini = ChatGoogleGenerativeAI(model="gemini-1.5-pro", temperature=0)
# gemini = ChatVertexAI(model="gemini-1.5-pro", temperature=0)

# TogetherAI : Llama-3.1-70B-Instruct-turbo
llama = ChatOpenAI(
    base_url="https://api.together.xyz/v1",
    api_key=os.environ["TOGETHER_API_KEY"],
    model="meta-llama/Meta-Llama-3.1-70B-Instruct-Turbo"
)

# # Llama 3.1
ollama = ChatOllama(model="llama3.1", temperature=0)

# # Qwen2.5 7B (í•œê¸€ì„±ëŠ¥ ë‚˜ì˜ì§€ ì•ŠìŒ.)
qwen = ChatOllama(model="qwen2.5:latest")
```

ê° LLM ê¸°ë°˜ìœ¼ë¡œ Agentë¥¼ ìƒì„±í•œë‹¤.

```python
from langchain.agents import create_tool_calling_agent

gpt_agent = create_tool_calling_agent(gpt, tools, prompt)
claude_agent = create_tool_calling_agent(claude, tools, prompt)
gemini_agent = create_tool_calling_agent(gemini, tools, prompt)
llama_agent = create_tool_calling_agent(llama, tools, prompt)
ollama_agent = create_tool_calling_agent(ollama, tools, prompt)
qwen_agent = create_tool_calling_agent(qwen, tools, prompt)

```

geminiëŠ” SDKë¥¼ ì„¤ì¹˜í•´ì•¼í•œë‹¤ê³  í•˜ë‹ˆ ê·¸ëƒ¥ ë„˜ì–´ê°€ì.

### AgentExecutor ìƒì„± í›„ã…œ ì‹¤í–‰ ë° ê²°ê³¼ í™•ì¸

```python
from langchain.agents import AgentExecutor

# gpt_agent ì‹¤í–‰
agent_executor = AgentExecutor(
    agent=gpt_agent,
    tools=tools,
    verbose=True,
    handle_parsing_errors=True
)

result = agent_executor.invoke ({
    "input": "ë¯¸êµ­ AI íˆ¬ìì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•´ì£¼ì„¸ìš”."
})

print("Agent ì‹¤í–‰ ê²°ê³¼ : \n", result["output"])


```

    [1m> Entering new AgentExecutor chain...[0m
    [32;1m[1;3m
    Invoking: `search_google_news` with `{'query': 'ë¯¸êµ­ AI íˆ¬ì'}`


    [0m[36;1m[1;3m[{'url': 'https://news.google.com/rss/articles/CBMiXkFVX3lxTFA3WU9zaU16TlNPWEVVMU1DaHVGQVhTeXhUNzZjSG5RVnRNbEo0Q2duV2JZdlhLOUdCUi1FVEMxVzZRSHVsX1ZlSTdMWUs2dmtORmFrMnZoYzYtVHBzeFHSAXZBVV95cUxPY24wZ05Td0JqSmRsSUZUOG1NZUtmTEdxTTI2aTV3d1RfbHdoYkFDQ1N2MnUtWWJjZmduTUI5RHBGRUs3NFJfUzd0TEgwSHlLSThneWdqSHNNV0tzcHZaazlFUTB3Rm12SG9UVGNaUGlqRm4zVHZR?oc=5', 'content': "íŠ¸ëŸ¼í”„, 'AI íŒ¨ê¶Œ' ì•¼ì‹¬ ë“œëŸ¬ë‚´...ì¤‘êµ­ ë…¸ê³¨ì  ê²¬ì œ - YTN"}, {'url': 'https://news.google.com/rss/articles/CBMiUEFVX3lxTE05TlIzaS0wSlN3V3NFZ0xtTnh1aVJYOWdSSVowQlE2YkZaaklTQk5IOF9FTE0xSDZYV25RYkV4dFl5RWVYQmFjbHJKMnV6RVQt0gFTQVVfeXFMTnFTNks0eXlSX0NESm1JX1hoOG1MbnlRcnM0ZjEtTDNOeTdxR0V6RVgxdmZEWXpOWGRNMzhSS1ZzblBzcEYwQl8yYlhQSjJ6bW1NMmc?oc=5', 'content': 'íŠ¸ëŸ¼í”„, AI ë¶„ì•¼ â€˜5ì²œì–µ ë‹¬ëŸ¬â€™ íˆ¬ì ê³„íš ë°œí‘œ - VOA Korea'}, {'url': 'https://news.google.com/rss/articles/CBMihAFBVV95cUxQN3FmTE9KZ2llMzVFa0dSQUdySlUyLU9DNTliaUgtN2hNSEd2SkRqYVJkUUoxOVNRbThOZ0dOY2FSZWpaTVZTaE9wTjN0UXZMLS1UcGtJRkxFRlhNVU1aay1PRnJUYXlWRHBvMXdGWHBDY2Zka05YUWRVZU9kYlJqMnNQQ1fSAZgBQVVfeXFMTUY1bzd4TlFIeUNMOWxLY2NGZ3NLcjJnZklxbmN4alUzV19HRW1xQTdYbUdvYlJ6dzc0RndwY0xRYWJhalZOWE5Bb3AxMHhDZHIybWtJRVgtQzhGZzV2OUhEODhRQWlPN25VcDM3NWducG5sV3NmQjdLZXd5UjBYNTUwSi1vQjE0MmJiaXZIVXdnM0lEZExsLUc?oc=5', 'content': 'íŠ¸ëŸ¼í”„ â€œç¾ AIì¸í”„ë¼ì— 5000ì–µ ë‹¬ëŸ¬ íˆ¬ìâ€ - ì¡°ì„ ì¼ë³´'}, {'url': 'https://news.google.com/rss/articles/CBMiWkFVX3lxTE1mYm1jSXlwV3hpZGhfRjg4eTF4Q3RwQXFSa0RpNkh2TFk2cUIyUEhEYTl1VWFOeWdyMHllZjMxODEycm1DTmt3ajVYWDhaLVozcUFQYWUxdHREQdIBVEFVX3lxTE5uaWY3LVpGT1MydXJhWjFLTERRcGpLZTJjbW9FNU9PWUtzV0c1bS1NRThsM1FUWHlSNHNkUmtILWU5VFFsSHhkd0w3WWdtQ2RIU21CbA?oc=5', 'content': '"ä¸­ \'ë”¥ì‹œí¬\' ë³´ë‹ˆâ€¦" íŠ¸ëŸ¼í”„ ë°œì–¸ì— ì—”ë¹„ë””ì•„ ê°œë¯¸ \'ì¶©ê²©\' - í•œêµ­ê²½ì œ'}, {'url': 'https://news.google.com/rss/articles/CBMiVkFVX3lxTE56ZXB0bWRUOHRsU2ktUW1fOFgxOWhYTjlGWjczbjVHRG9vOEF2VFY5dUszTTBjVkxfZXJoRGNoTlpVRWZoV00xTFlsRlE0bzFrbmJDVjFn?oc=5', 'content': 'íŠ¸ëŸ¼í”„ "AIì— 718ì¡° íˆ¬ì"â€¦ë¯¸âˆ™ì¤‘ íŒ¨ê¶Œ 3ì°¨ëŒ€ì „, éŸ“ì—” í˜¸ì¬ - ì¤‘ì•™ì¼ë³´'}][0m[32;1m[1;3më‹¤ìŒì€ ë¯¸êµ­ AI íˆ¬ìì™€ ê´€ë ¨ëœ ìµœê·¼ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤:

    1. [íŠ¸ëŸ¼í”„, 'AI íŒ¨ê¶Œ' ì•¼ì‹¬ ë“œëŸ¬ë‚´...ì¤‘êµ­ ë…¸ê³¨ì  ê²¬ì œ - YTN](https://news.google.com/rss/articles/CBMiXkFVX3lxTFA3WU9zaU16TlNPWEVVMU1DaHVGQVhTeXhUNzZjSG5RVnRNbEo0Q2duV2JZdlhLOUdCUi1FVEMxVzZRSHVsX1ZlSTdMWUs2dmtORmFrMnZoYzYtVHBzeFHSAXZBVV95cUxPY24wZ05Td0JqSmRsSUZUOG1NZUtmTEdxTTI2aTV3d1RfbHdoYkFDQ1N2MnUtWWJjZmduTUI5RHBGRUs3NFJfUzd0TEgwSHlLSThneWdqSHNNV0tzcHZaazlFUTB3Rm12SG9UVGNaUGlqRm4zVHZR?oc=5)

    2. [íŠ¸ëŸ¼í”„, AI ë¶„ì•¼ â€˜5ì²œì–µ ë‹¬ëŸ¬â€™ íˆ¬ì ê³„íš ë°œí‘œ - VOA Korea](https://news.google.com/rss/articles/CBMiUEFVX3lxTE05TlIzaS0wSlN3V3NFZ0xtTnh1aVJYOWdSSVowQlE2YkZaaklTQk5IOF9FTE0xSDZYV25RYkV4dFl5RWVYQmFjbHJKMnV6RVQt0gFTQVVfeXFMTnFTNks0eXlSX0NESm1JX1hoOG1MbnlRcnM0ZjEtTDNOeTdxR0V6RVgxdmZEWXpOWGRNMzhSS1ZzblBzcEYwQl8yYlhQSjJ6bW1NMmc?oc=5)

    3. [íŠ¸ëŸ¼í”„ â€œç¾ AIì¸í”„ë¼ì— 5000ì–µ ë‹¬ëŸ¬ íˆ¬ìâ€ - ì¡°ì„ ì¼ë³´](https://news.google.com/rss/articles/CBMihAFBVV95cUxQN3FmTE9KZ2llMzVFa0dSQUdySlUyLU9DNTliaUgtN2hNSEd2SkRqYVJkUUoxOVNRbThOZ0dOY2FSZWpaTVZTaE9wTjN0UXZMLS1UcGtJRkxFRlhNVU1aay1PRnJUYXlWRHBvMXdGWHBDY2Zka05YUWRVZU9kYlJqMnNQQ1fSAZgBQVVfeXFMTUY1bzd4TlFIeUNMOWxLY2NGZ3NLcjJnZklxbmN4alUzV19HRW1xQTdYbUdvYlJ6dzc0RndwY0xRYWJhalZOWE5Bb3AxMHhDZHIybWtJRVgtQzhGZzV2OUhEODhRQWlPN25VcDM3NWducG5sV3NmQjdLZXd5UjBYNTUwSi1vQjE0MmJiaXZIVXdnM0lEZExsLUc?oc=5)

    4. [\"ä¸­ 'ë”¥ì‹œí¬' ë³´ë‹ˆâ€¦\" íŠ¸ëŸ¼í”„ ë°œì–¸ì— ì—”ë¹„ë””ì•„ ê°œë¯¸ 'ì¶©ê²©' - í•œêµ­ê²½ì œ](https://news.google.com/rss/articles/CBMiWkFVX3lxTE1mYm1jSXlwV3hpZGhfRjg4eTF4Q3RwQXFSa0RpNkh2TFk2cUIyUEhEYTl1VWFOeWdyMHllZjMxODEycm1DTmt3ajVYWDhaLVozcUFQYWUxdHREQdIBVEFVX3lxTE5uaWY3LVpGT1MydXJhWjFLTERRcGpLZTJjbW9FNU9PWUtzV0c1bS1NRThsM1FUWHlSNHNkUmtILWU5VFFsSHhkd0w3WWdtQ2RIU21CbA?oc=5)

    5. [íŠ¸ëŸ¼í”„ \"AIì— 718ì¡° íˆ¬ì\"â€¦ë¯¸âˆ™ì¤‘ íŒ¨ê¶Œ 3ì°¨ëŒ€ì „, éŸ“ì—” í˜¸ì¬ - ì¤‘ì•™ì¼ë³´](https://news.google.com/rss/articles/CBMiVkFVX3lxTE56ZXB0bWRUOHRsU2ktUW1fOFgxOWhYTjlGWjczbjVHRG9vOEF2VFY5dUszTTBjVkxfZXJoRGNoTlpVRWZoV00xTFlsRlE0bzFrbmJDVjFn?oc=5)

    ì´ ê¸°ì‚¬ë“¤ì€ íŠ¸ëŸ¼í”„ ì „ ëŒ€í†µë ¹ì˜ AI ë¶„ì•¼ì— ëŒ€í•œ íˆ¬ì ê³„íšê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.[0m

    [1m> Finished chain.[0m
    Agent ì‹¤í–‰ ê²°ê³¼ :
     ë‹¤ìŒì€ ë¯¸êµ­ AI íˆ¬ìì™€ ê´€ë ¨ëœ ìµœê·¼ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤:

    1. [íŠ¸ëŸ¼í”„, 'AI íŒ¨ê¶Œ' ì•¼ì‹¬ ë“œëŸ¬ë‚´...ì¤‘êµ­ ë…¸ê³¨ì  ê²¬ì œ - YTN](https://news.google.com/rss/articles/CBMiXkFVX3lxTFA3WU9zaU16TlNPWEVVMU1DaHVGQVhTeXhUNzZjSG5RVnRNbEo0Q2duV2JZdlhLOUdCUi1FVEMxVzZRSHVsX1ZlSTdMWUs2dmtORmFrMnZoYzYtVHBzeFHSAXZBVV95cUxPY24wZ05Td0JqSmRsSUZUOG1NZUtmTEdxTTI2aTV3d1RfbHdoYkFDQ1N2MnUtWWJjZmduTUI5RHBGRUs3NFJfUzd0TEgwSHlLSThneWdqSHNNV0tzcHZaazlFUTB3Rm12SG9UVGNaUGlqRm4zVHZR?oc=5)

    2. [íŠ¸ëŸ¼í”„, AI ë¶„ì•¼ â€˜5ì²œì–µ ë‹¬ëŸ¬â€™ íˆ¬ì ê³„íš ë°œí‘œ - VOA Korea](https://news.google.com/rss/articles/CBMiUEFVX3lxTE05TlIzaS0wSlN3V3NFZ0xtTnh1aVJYOWdSSVowQlE2YkZaaklTQk5IOF9FTE0xSDZYV25RYkV4dFl5RWVYQmFjbHJKMnV6RVQt0gFTQVVfeXFMTnFTNks0eXlSX0NESm1JX1hoOG1MbnlRcnM0ZjEtTDNOeTdxR0V6RVgxdmZEWXpOWGRNMzhSS1ZzblBzcEYwQl8yYlhQSjJ6bW1NMmc?oc=5)

    3. [íŠ¸ëŸ¼í”„ â€œç¾ AIì¸í”„ë¼ì— 5000ì–µ ë‹¬ëŸ¬ íˆ¬ìâ€ - ì¡°ì„ ì¼ë³´](https://news.google.com/rss/articles/CBMihAFBVV95cUxQN3FmTE9KZ2llMzVFa0dSQUdySlUyLU9DNTliaUgtN2hNSEd2SkRqYVJkUUoxOVNRbThOZ0dOY2FSZWpaTVZTaE9wTjN0UXZMLS1UcGtJRkxFRlhNVU1aay1PRnJUYXlWRHBvMXdGWHBDY2Zka05YUWRVZU9kYlJqMnNQQ1fSAZgBQVVfeXFMTUY1bzd4TlFIeUNMOWxLY2NGZ3NLcjJnZklxbmN4alUzV19HRW1xQTdYbUdvYlJ6dzc0RndwY0xRYWJhalZOWE5Bb3AxMHhDZHIybWtJRVgtQzhGZzV2OUhEODhRQWlPN25VcDM3NWducG5sV3NmQjdLZXd5UjBYNTUwSi1vQjE0MmJiaXZIVXdnM0lEZExsLUc?oc=5)

    4. [\"ä¸­ 'ë”¥ì‹œí¬' ë³´ë‹ˆâ€¦\" íŠ¸ëŸ¼í”„ ë°œì–¸ì— ì—”ë¹„ë””ì•„ ê°œë¯¸ 'ì¶©ê²©' - í•œêµ­ê²½ì œ](https://news.google.com/rss/articles/CBMiWkFVX3lxTE1mYm1jSXlwV3hpZGhfRjg4eTF4Q3RwQXFSa0RpNkh2TFk2cUIyUEhEYTl1VWFOeWdyMHllZjMxODEycm1DTmt3ajVYWDhaLVozcUFQYWUxdHREQdIBVEFVX3lxTE5uaWY3LVpGT1MydXJhWjFLTERRcGpLZTJjbW9FNU9PWUtzV0c1bS1NRThsM1FUWHlSNHNkUmtILWU5VFFsSHhkd0w3WWdtQ2RIU21CbA?oc=5)

    5. [íŠ¸ëŸ¼í”„ \"AIì— 718ì¡° íˆ¬ì\"â€¦ë¯¸âˆ™ì¤‘ íŒ¨ê¶Œ 3ì°¨ëŒ€ì „, éŸ“ì—” í˜¸ì¬ - ì¤‘ì•™ì¼ë³´](https://news.google.com/rss/articles/CBMiVkFVX3lxTE56ZXB0bWRUOHRsU2ktUW1fOFgxOWhYTjlGWjczbjVHRG9vOEF2VFY5dUszTTBjVkxfZXJoRGNoTlpVRWZoV00xTFlsRlE0bzFrbmJDVjFn?oc=5)

    ì´ ê¸°ì‚¬ë“¤ì€ íŠ¸ëŸ¼í”„ ì „ ëŒ€í†µë ¹ì˜ AI ë¶„ì•¼ì— ëŒ€í•œ íˆ¬ì ê³„íšê³¼ ê´€ë ¨ëœ ë‚´ìš©ì„ ë‹¤ë£¨ê³  ìˆìŠµë‹ˆë‹¤.

ì´ì œ ë‹¤ì–‘í•œ llmì„ ì‚¬ìš©í•˜ì—¬ agentë¥¼ ì‹¤í–‰í•´ë³´ì.

ë‹¤ìŒì€ ì…ë ¥ë°›ì€ llmì„ ì‚¬ìš©í•˜ì—¬ Agentë¥¼ ìƒì„±í•˜ê³  ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜ì´ë‹¤.

```python
def execute_agent(llm, tools, input_text, label):
    agent = create_tool_calling_agent(llm, tools, prompt)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=False)
    result = executor.invoke({"input": input_text})
    print(f"[ '{label}' ì‹¤í–‰ ê²°ê³¼ ]")

    if isinstance(result["output"], list) and len(result["output"]) > 0:
        for item in result ["output"]:
            if "text" in item:
                print(item["text"])
    elif isinstance(result["output"], str):
        print(result["output"])
    else:
        print(result["output"])

```

ê° LLM ë³„ë¡œ ì—ì´ì „íŠ¸ë¥¼ ìƒì„±, ì‹¤í–‰í•˜ì—¬ ê²°ê³¼ë¥¼ ì¶œë ¥.

```python
query = (
    "AI íˆ¬ìì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³ , ê²°ê³¼ë¥¼ Instagram ê²Œì‹œê¸€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”."
)
```

```python

# gpt
execute_agent(gpt, tools, query, "gpt")
```

    [ 'gpt' ì‹¤í–‰ ê²°ê³¼ ]
    ğŸŒŸ **AI íˆ¬ì ë™í–¥** ğŸŒŸ

    1ï¸âƒ£ **MSì™€ ë©”íƒ€**: ë©”íƒ€ê°€ AIì— ëŒ€í•œ ì§€ì†ì ì¸ íˆ¬ìë¥¼ í†µí•´ ì£¼ê°€ ìƒìŠ¹ì„¸ë¥¼ ì´ì–´ê°€ê³  ìˆë‹¤ëŠ” ì†Œì‹ì…ë‹ˆë‹¤! ğŸ¤‘ [ìì„¸íˆ ë³´ê¸°](https://news.google.com/rss/articles/CBMiWkFVX3lxTE8yeEdfOWxJSmtHSUJYS0ZXVDJaU1RWUTNzU01EVkJQaFpOREZNRE95REpjWU9YR3ZJTlAyODJVdzU2djNZSW5kUjRpVkdid3lfYU9USlNlLTRuZ9IBVEFVX3lxTE1xVzJZZ1lRQ3R6UnlwYkkwS1dsSVBWV2o5TmZoTjBDMGtRanZzMENMMlE2bE9PbHpGMjVsQ0ZOYlk1Wk96bWJGMzlkQ3pqVVZvTndmcw?oc=5)

    2ï¸âƒ£ **íŠ¸ëŸ¼í”„ ë°œ AI íˆ¬ì**: AI íˆ¬ìê°€ í•œêµ­ ê¸°ì—…ì— ìƒˆë¡œìš´ ê¸°íšŒë¥¼ ì œê³µí•  ìˆ˜ ìˆë‹¤ëŠ” ì „ë§ì´ ë‚˜ì˜¤ê³  ìˆìŠµë‹ˆë‹¤. ì „ë ¥ ì†”ë£¨ì…˜ ì‹œì¥ì´ ì£¼ëª©ë°›ê³  ìˆë„¤ìš”! ğŸ” [ìì„¸íˆ ë³´ê¸°](https://news.google.com/rss/articles/CBMiW0FVX3lxTE4zbUlUMW5PZ2xBWVBXQjRrdWM1ZlRlN3BoWE1VRkd3cHRlcTVKSEEtLUZrMWlKc2NkRFY5dmJ3YmlmUTctd2Q4SzNxQnJBZnJMYzJxOVo1WjB1em_SAV5BVV95cUxQWjVzN2t0ejY2akhjZGVGbnpRRWw3ZWUwckFLc3U3NXkwaFcxaTVCRWJHY0M0STNhMU1heURwWmlXa285YnJqa2h5NFdkamVEejhwQnpVNE9uVS1aWk9B?oc=5)

    3ï¸âƒ£ **ETFì™€ AI íˆ¬ì**: ê¸ˆë¦¬ í•˜ë½ê¸°ì—ëŠ” ETFì™€ AI íˆ¬ìì˜ ì¡°í•©ìœ¼ë¡œ ì „ëµì  ìì‚° ë°°ë¶„ ê°€ì´ë“œê°€ ê°•ì¡°ë˜ê³  ìˆìŠµë‹ˆë‹¤! ğŸ“Š [ìì„¸íˆ ë³´ê¸°](https://news.google.com/rss/articles/CBMiVkFVX3lxTFA4WnRkSE1kZnUteWhMdmJWSWFEWUs3Y0tTTVFjc0M2YmNNOEM5b0RHa3pDN0dneXp6ZG9tcGNiQjMtYXF0X2l6a3VoMWlrTHdSVDgyLXlR?oc=5)

    4ï¸âƒ£ **ì• í”Œ ì£¼ê°€ ê¸‰ë“±**: AI íˆ¬ìì— ìƒëŒ€ì ìœ¼ë¡œ ì ê²Œ ì°¸ì—¬í–ˆë‹¤ëŠ” ì´ìœ ë¡œ ì• í”Œì˜ ì£¼ê°€ê°€ 3% ê¸‰ë“±í•˜ë©° ì‹œê°€ì´ì•¡ 1ìœ„ë¥¼ íƒˆí™˜í–ˆìŠµë‹ˆë‹¤! ğŸ [ìì„¸íˆ ë³´ê¸°](https://news.google.com/rss/articles/CBMiWEFVX3lxTE5IYk80R1pzV084dlpXODE3ZWRLTGFhZ0NaWmJoTGRIVE00OWwzcVBTTElUanJjRmtDMmVZV20xUkVoaEVFRnVxYTV1VFJ4T1B2dkxMM1cwUDQ?oc=5)

    5ï¸âƒ£ **ì–´ë‹ ì„œí”„ë¼ì´ì¦ˆ ë©”íƒ€**: ë©”íƒ€ê°€ ì–´ë‹ ì„œí”„ë¼ì´ì¦ˆë¡œ ì¸í•´ ì£¼ê°€ê°€ 4% ìƒìŠ¹í•˜ë©° AI ì¸í”„ë¼ì— ëŒ€í•œ ì§€ì†ì ì¸ íˆ¬ì ì˜ì‚¬ë¥¼ ë°í˜”ìŠµë‹ˆë‹¤! ğŸ’¹ [ìì„¸íˆ ë³´ê¸°](https://news.google.com/rss/articles/CBMiUkFVX3lxTE5kVkZWUWxKZXRuUzQxRkFOc2pVNzdDMkxFUzFGeE51QUZrb2tMMVo5cUptTExXS01jZEFlTXBQUWlTRXBmYUhfUjNXc24xU1daQkE?oc=5)

    ğŸ“ˆ **AI íˆ¬ì ì—´í’ì´ ê³„ì†ë˜ê³  ìˆìŠµë‹ˆë‹¤! ì–´ë–¤ ê¸°ì—…ì´ ë‹¤ìŒ ì£¼ì‹ì˜ ì£¼ì¸ê³µì´ ë ì§€ ê¸°ëŒ€ë©ë‹ˆë‹¤!**

```python
# claude
# execute_agent(claude, tools, query, "claude")
```

claudeëŠ” credit balance ê°€ ë¶€ì¡±í•˜ì—¬ ì‹¤í–‰ì´ ì•ˆë˜ë‹ˆ ê·¸ëƒ¥ ë„˜ì–´ê°€ì.

```python
# google gemini
execute_agent(gemini, tools, query, "gemini")
```

    ---------------------------------------------------------------------------

    InvalidArgument                           Traceback (most recent call last)

    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:178, in _chat_with_retry.<locals>._chat_with_retry(**kwargs)
        177 try:
    --> 178     return generation_method(**kwargs)
        179 # Do not retry for these errors.


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/ai/generativelanguage_v1beta/services/generative_service/client.py:1143, in GenerativeServiceClient.stream_generate_content(self, request, model, contents, retry, timeout, metadata)
       1142 # Send the request.
    -> 1143 response = rpc(
       1144     request,
       1145     retry=retry,
       1146     timeout=timeout,
       1147     metadata=metadata,
       1148 )
       1150 # Done; return the response.


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/gapic_v1/method.py:131, in _GapicCallable.__call__(self, timeout, retry, compression, *args, **kwargs)
        129     kwargs["compression"] = compression
    --> 131 return wrapped_func(*args, **kwargs)


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:293, in Retry.__call__.<locals>.retry_wrapped_func(*args, **kwargs)
        290 sleep_generator = exponential_sleep_generator(
        291     self._initial, self._maximum, multiplier=self._multiplier
        292 )
    --> 293 return retry_target(
        294     target,
        295     self._predicate,
        296     sleep_generator,
        297     timeout=self._timeout,
        298     on_error=on_error,
        299 )


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:153, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)
        151 except Exception as exc:
        152     # defer to shared logic for handling errors
    --> 153     _retry_error_helper(
        154         exc,
        155         deadline,
        156         sleep,
        157         error_list,
        158         predicate,
        159         on_error,
        160         exception_factory,
        161         timeout,
        162     )
        163     # if exception not raised, sleep before next attempt


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/retry/retry_base.py:212, in _retry_error_helper(exc, deadline, next_sleep, error_list, predicate_fn, on_error_fn, exc_factory_fn, original_timeout)
        207     final_exc, source_exc = exc_factory_fn(
        208         error_list,
        209         RetryFailureReason.NON_RETRYABLE_ERROR,
        210         original_timeout,
        211     )
    --> 212     raise final_exc from source_exc
        213 if on_error_fn is not None:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/retry/retry_unary.py:144, in retry_target(target, predicate, sleep_generator, timeout, on_error, exception_factory, **kwargs)
        143 try:
    --> 144     result = target()
        145     if inspect.isawaitable(result):


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/timeout.py:130, in TimeToDeadlineTimeout.__call__.<locals>.func_with_timeout(*args, **kwargs)
        128     kwargs["timeout"] = remaining_timeout
    --> 130 return func(*args, **kwargs)


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/google/api_core/grpc_helpers.py:174, in _wrap_stream_errors.<locals>.error_remapped_callable(*args, **kwargs)
        173 except grpc.RpcError as exc:
    --> 174     raise exceptions.from_grpc_error(exc) from exc


    InvalidArgument: 400 * GenerateContentRequest.contents[2].parts[0].function_response.name: Name cannot be empty.



    The above exception was the direct cause of the following exception:


    ChatGoogleGenerativeAIError               Traceback (most recent call last)

    Cell In[37], line 2
          1 # google gemini
    ----> 2 execute_agent(gemini, tools, query, "gemini")


    Cell In[33], line 4, in execute_agent(llm, tools, input_text, label)
          2 agent = create_tool_calling_agent(llm, tools, prompt)
          3 executor = AgentExecutor(agent=agent, tools=tools, verbose=False)
    ----> 4 result = executor.invoke({"input": input_text})
          5 print(f"[ '{label}' ì‹¤í–‰ ê²°ê³¼ ]")
          7 if isinstance(result["output"], list) and len(result["output"]) > 0:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain/chains/base.py:170, in Chain.invoke(self, input, config, **kwargs)
        168 except BaseException as e:
        169     run_manager.on_chain_error(e)
    --> 170     raise e
        171 run_manager.on_chain_end(outputs)
        173 if include_run_info:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain/chains/base.py:160, in Chain.invoke(self, input, config, **kwargs)
        157 try:
        158     self._validate_inputs(inputs)
        159     outputs = (
    --> 160         self._call(inputs, run_manager=run_manager)
        161         if new_arg_supported
        162         else self._call(inputs)
        163     )
        165     final_outputs: Dict[str, Any] = self.prep_outputs(
        166         inputs, outputs, return_only_outputs
        167     )
        168 except BaseException as e:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain/agents/agent.py:1624, in AgentExecutor._call(self, inputs, run_manager)
       1622 # We now enter the agent loop (until it returns something).
       1623 while self._should_continue(iterations, time_elapsed):
    -> 1624     next_step_output = self._take_next_step(
       1625         name_to_tool_map,
       1626         color_mapping,
       1627         inputs,
       1628         intermediate_steps,
       1629         run_manager=run_manager,
       1630     )
       1631     if isinstance(next_step_output, AgentFinish):
       1632         return self._return(
       1633             next_step_output, intermediate_steps, run_manager=run_manager
       1634         )


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain/agents/agent.py:1332, in AgentExecutor._take_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)
       1321 def _take_next_step(
       1322     self,
       1323     name_to_tool_map: Dict[str, BaseTool],
       (...)
       1327     run_manager: Optional[CallbackManagerForChainRun] = None,
       1328 ) -> Union[AgentFinish, List[Tuple[AgentAction, str]]]:
       1329     return self._consume_next_step(
       1330         [
       1331             a
    -> 1332             for a in self._iter_next_step(
       1333                 name_to_tool_map,
       1334                 color_mapping,
       1335                 inputs,
       1336                 intermediate_steps,
       1337                 run_manager,
       1338             )
       1339         ]
       1340     )


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain/agents/agent.py:1358, in AgentExecutor._iter_next_step(self, name_to_tool_map, color_mapping, inputs, intermediate_steps, run_manager)
       1355     intermediate_steps = self._prepare_intermediate_steps(intermediate_steps)
       1357     # Call the LLM to see what to do.
    -> 1358     output = self._action_agent.plan(
       1359         intermediate_steps,
       1360         callbacks=run_manager.get_child() if run_manager else None,
       1361         **inputs,
       1362     )
       1363 except OutputParserException as e:
       1364     if isinstance(self.handle_parsing_errors, bool):


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain/agents/agent.py:581, in RunnableMultiActionAgent.plan(self, intermediate_steps, callbacks, **kwargs)
        573 final_output: Any = None
        574 if self.stream_runnable:
        575     # Use streaming to make sure that the underlying LLM is invoked in a
        576     # streaming
       (...)
        579     # Because the response from the plan is not a generator, we need to
        580     # accumulate the output into final output and return that.
    --> 581     for chunk in self.runnable.stream(inputs, config={"callbacks": callbacks}):
        582         if final_output is None:
        583             final_output = chunk


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:3401, in RunnableSequence.stream(self, input, config, **kwargs)
       3395 def stream(
       3396     self,
       3397     input: Input,
       3398     config: Optional[RunnableConfig] = None,
       3399     **kwargs: Optional[Any],
       3400 ) -> Iterator[Output]:
    -> 3401     yield from self.transform(iter([input]), config, **kwargs)


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:3388, in RunnableSequence.transform(self, input, config, **kwargs)
       3382 def transform(
       3383     self,
       3384     input: Iterator[Input],
       3385     config: Optional[RunnableConfig] = None,
       3386     **kwargs: Optional[Any],
       3387 ) -> Iterator[Output]:
    -> 3388     yield from self._transform_stream_with_config(
       3389         input,
       3390         self._transform,
       3391         patch_config(config, run_name=(config or {}).get("run_name") or self.name),
       3392         **kwargs,
       3393     )


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:2188, in Runnable._transform_stream_with_config(self, input, transformer, config, run_type, **kwargs)
       2186 try:
       2187     while True:
    -> 2188         chunk: Output = context.run(next, iterator)  # type: ignore
       2189         yield chunk
       2190         if final_output_supported:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:3351, in RunnableSequence._transform(self, input, run_manager, config, **kwargs)
       3348     else:
       3349         final_pipeline = step.transform(final_pipeline, config)
    -> 3351 yield from final_pipeline


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:1406, in Runnable.transform(self, input, config, **kwargs)
       1403 final: Input
       1404 got_first_val = False
    -> 1406 for ichunk in input:
       1407     # The default implementation of transform is to buffer input and
       1408     # then call stream.
       1409     # It'll attempt to gather all input into a single chunk using
       1410     # the `+` operator.
       1411     # If the input is not addable, then we'll assume that we can
       1412     # only operate on the last chunk,
       1413     # and we'll iterate until we get to the last chunk.
       1414     if not got_first_val:
       1415         final = ichunk


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:5559, in RunnableBindingBase.transform(self, input, config, **kwargs)
       5553 def transform(
       5554     self,
       5555     input: Iterator[Input],
       5556     config: Optional[RunnableConfig] = None,
       5557     **kwargs: Any,
       5558 ) -> Iterator[Output]:
    -> 5559     yield from self.bound.transform(
       5560         input,
       5561         self._merge_configs(config),
       5562         **{**self.kwargs, **kwargs},
       5563     )


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/runnables/base.py:1424, in Runnable.transform(self, input, config, **kwargs)
       1421             final = ichunk
       1423 if got_first_val:
    -> 1424     yield from self.stream(final, config, **kwargs)


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_core/language_models/chat_models.py:398, in BaseChatModel.stream(self, input, config, stop, **kwargs)
        395     self.rate_limiter.acquire(blocking=True)
        397 try:
    --> 398     for chunk in self._stream(messages, stop=stop, **kwargs):
        399         if chunk.message.id is None:
        400             chunk.message.id = f"run-{run_manager.run_id}"


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:1034, in ChatGoogleGenerativeAI._stream(self, messages, stop, run_manager, tools, functions, safety_settings, tool_config, generation_config, cached_content, tool_choice, **kwargs)
       1008 def _stream(
       1009     self,
       1010     messages: List[BaseMessage],
       (...)
       1021     **kwargs: Any,
       1022 ) -> Iterator[ChatGenerationChunk]:
       1023     request = self._prepare_request(
       1024         messages,
       1025         stop=stop,
       (...)
       1032         tool_choice=tool_choice,
       1033     )
    -> 1034     response: GenerateContentResponse = _chat_with_retry(
       1035         request=request,
       1036         generation_method=self.client.stream_generate_content,
       1037         **kwargs,
       1038         metadata=self.default_metadata,
       1039     )
       1041     prev_usage_metadata: UsageMetadata | None = None
       1042     for chunk in response:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:196, in _chat_with_retry(generation_method, **kwargs)
        193     except Exception as e:
        194         raise e
    --> 196 return _chat_with_retry(**kwargs)


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/tenacity/__init__.py:336, in BaseRetrying.wraps.<locals>.wrapped_f(*args, **kw)
        334 copy = self.copy()
        335 wrapped_f.statistics = copy.statistics  # type: ignore[attr-defined]
    --> 336 return copy(f, *args, **kw)


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/tenacity/__init__.py:475, in Retrying.__call__(self, fn, *args, **kwargs)
        473 retry_state = RetryCallState(retry_object=self, fn=fn, args=args, kwargs=kwargs)
        474 while True:
    --> 475     do = self.iter(retry_state=retry_state)
        476     if isinstance(do, DoAttempt):
        477         try:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/tenacity/__init__.py:376, in BaseRetrying.iter(self, retry_state)
        374 result = None
        375 for action in self.iter_state.actions:
    --> 376     result = action(retry_state)
        377 return result


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/tenacity/__init__.py:398, in BaseRetrying._post_retry_check_actions.<locals>.<lambda>(rs)
        396 def _post_retry_check_actions(self, retry_state: "RetryCallState") -> None:
        397     if not (self.iter_state.is_explicit_retry or self.iter_state.retry_run_result):
    --> 398         self._add_action_func(lambda rs: rs.outcome.result())
        399         return
        401     if self.after is not None:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/concurrent/futures/_base.py:449, in Future.result(self, timeout)
        447     raise CancelledError()
        448 elif self._state == FINISHED:
    --> 449     return self.__get_result()
        451 self._condition.wait(timeout)
        453 if self._state in [CANCELLED, CANCELLED_AND_NOTIFIED]:


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/concurrent/futures/_base.py:401, in Future.__get_result(self)
        399 if self._exception:
        400     try:
    --> 401         raise self._exception
        402     finally:
        403         # Break a reference cycle with the exception in self._exception
        404         self = None


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/tenacity/__init__.py:478, in Retrying.__call__(self, fn, *args, **kwargs)
        476 if isinstance(do, DoAttempt):
        477     try:
    --> 478         result = fn(*args, **kwargs)
        479     except BaseException:  # noqa: B902
        480         retry_state.set_exception(sys.exc_info())  # type: ignore[arg-type]


    File /opt/homebrew/Caskroom/miniforge/base/envs/test_langchain/lib/python3.12/site-packages/langchain_google_genai/chat_models.py:190, in _chat_with_retry.<locals>._chat_with_retry(**kwargs)
        187         raise ValueError(error_msg)
        189 except google.api_core.exceptions.InvalidArgument as e:
    --> 190     raise ChatGoogleGenerativeAIError(
        191         f"Invalid argument provided to Gemini: {e}"
        192     ) from e
        193 except Exception as e:
        194     raise e


    ChatGoogleGenerativeAIError: Invalid argument provided to Gemini: 400 * GenerateContentRequest.contents[2].parts[0].function_response.name: Name cannot be empty.

```python

# llama3.1 70B (Together.ai)
execute_agent(
    llama,
    tools,
    "Search AI related news and write it in Instagram post format",
    "llama3.1 70B",
)
```

    [ 'llama3.1 70B' ì‹¤í–‰ ê²°ê³¼ ]
    "Hey everyone!

    Want to stay up-to-date on the latest AI news?

    Here are some recent articles from Google News:

    1. "ë”¥ì‹œí¬ê°€ 'ì±—GPT' ë² ë‚€ ì¦ê±° ìˆì–´"..."ì˜¤í”ˆAI, í•©ì„±ë°ì´í„° í™œìš© ë°©ì§€ ì˜ˆê³ " - AIíƒ€ì„ìŠ¤
    2. "ì˜¤í”ˆAI ë“± â€³ë”¥ì‹œí¬ ì¡°ì‚¬â€¦ë°ì´í„° ë¬´ë‹¨ ìˆ˜ì§‘ ì—¬ë¶€â€³" - ì—°í•©ë‰´ìŠ¤TV
    3. "AIë¡œ ì œì¡°ì—… ì•ˆì „ì‚¬ê³  ì˜ˆë°©í•œë‹¤â€¦ì‚°ì—…ë¶€, R&D ì‚¬ì—… ê³µëª¨" - ì—°í•©ë‰´ìŠ¤
    4. "ì‚°ì—…ë¶€, ì œì¡°ì•ˆì „ê³ ë„í™”ê¸°ìˆ ê°œë°œì‚¬ì—… ê³µê³ ...AIë¡œ ì œì¡°ì—… ì•ˆì „ì‚¬ê³  ì˜ˆë°©í•œë‹¤" - AIíƒ€ì„ìŠ¤
    5. "AIë¡œ ì œì¡°ì—… ì‚°ì¬ ìœ„í—˜ ì˜ˆë°©â€¦ì‚°ì—…ë¶€, ë°°í„°ë¦¬Â·ì„ìœ í™”í•™ ë¶„ì•¼ ê³¼ì œ ê³µëª¨" - ë‰´ì‹œìŠ¤

    Check out these articles to learn more about the latest developments in AI!

    #AI #GoogleNews #LatestNews"

```python
# llama3.1 8B (ollama)
execute_agent(ollama, tools, query, "llama3.1(Ollama)")

```

    [ 'llama3.1(Ollama)' ì‹¤í–‰ ê²°ê³¼ ]
    **AI íˆ¬ì ë‰´ìŠ¤**

    *   **íƒœêµ­ BOI, 5ì–µ ë‹¬ëŸ¬ ê·œëª¨ì˜ íˆ¬ì ìŠ¹ì¸** íƒœêµ­ BOIëŠ” TikTokì˜ ë°ì´í„° í˜¸ìŠ¤íŒ… í”„ë¡œì íŠ¸ë¥¼ í¬í•¨í•œ ì´ 5ì–µ ë‹¬ëŸ¬ ê·œëª¨ì˜ íˆ¬ìë¥¼ ìŠ¹ì¸í–ˆë‹¤. ë˜í•œ ë°”ì´ì˜¤ ê²½ì œì— ëŒ€í•œ í˜ì‹ ì ì¸ ì¸ì„¼í‹°ë¸Œë¥¼ ë°œí‘œí–ˆë‹¤.
    *   **42ì¡° ì› ê·œëª¨ì˜ íƒ„ì•½, 'AI' ì „ë¶€** IT ì¡°ì„ ì€ 42ì¡° ì› ê·œëª¨ì˜ íƒ„ì•½ì´ ëª¨ë‘ AI ê´€ë ¨ ì‚¬ì—…ì´ë¼ê³  ë³´ë„í–ˆë‹¤.
    *   **íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹, ë¯¸êµ­ ë‚´ì—ì„œì˜ AI ê´€ë ¨ ì‚¬ì—…ì— 77ì¡° ì—” ì„¤ë¹„ íˆ¬ì ë°œí‘œ** nhk.or.jpëŠ” íŠ¸ëŸ¼í”„ ëŒ€í†µë ¹ì´ ë¯¸êµ­ ë‚´ì—ì„œì˜ AI ê´€ë ¨ ì‚¬ì—…ì— 77ì¡° ì—” ê·œëª¨ì˜ ì„¤ë¹„ íˆ¬ìë¥¼ ë°œí‘œí–ˆë‹¤ê³  ë³´ë„í–ˆë‹¤.
    *   **íˆ¬ìŠ¤ ì‹œíë¦¬í‹°, 2025ë…„ ë¯¸êµ­ íˆ¬ì ì „ë§ ë°œí‘œ** KIPOSTëŠ” íˆ¬ìŠ¤ ì‹œíë¦¬í‹°ê°€ 2025ë…„ ë¯¸êµ­ íˆ¬ì ì „ë§ì„ ë°œí‘œí–ˆë‹¤ê³  ë³´ë„í–ˆë‹¤.
    *   **ì™¸ì‹ì—… ëŒ“ê¸€ ê´€ë¦¬ AI' ë¥´ëª½, ë¸”ë£¨í¬ì¸íŠ¸ì—ì„œ ì‹œë“œíˆ¬ì ìœ ì¹˜** ìŠ¤íƒ€íŠ¸ì—…ì—”(StartupN)ì€ ì™¸ì‹ì—… ëŒ“ê¸€ ê´€ë¦¬ AIì¸ ë¥´ëª½ì´ ë¸”ë£¨í¬ì¸íŠ¸ì—ì„œ ì‹œë“œ íˆ¬ìë¥¼ ìœ ì¹˜í–ˆë‹¤ê³  ë³´ë„í–ˆë‹¤.

```python

# qwen2.5 7B (ollama)
query = "AI íˆ¬ìì™€ ê´€ë ¨ëœ ë‰´ìŠ¤ë¥¼ ê²€ìƒ‰í•˜ê³ , ê²°ê³¼ë¥¼ Instagram ê²Œì‹œê¸€ í˜•ì‹ìœ¼ë¡œ ì‘ì„±í•´ ì£¼ì„¸ìš”. í•œê¸€ë¡œ ë‹µë³€í•˜ì„¸ìš”!"

execute_agent(qwen, tools, query, "qwen2.5(Ollama)")
```

    [ 'qwen2.5(Ollama)' ì‹¤í–‰ ê²°ê³¼ ]
    Instagram ê²Œì‹œê¸€:

    ğŸš€ AI íˆ¬ì ë‰´ìŠ¤ ğŸš€

    1. MS, ë©”íƒ€ "AI íˆ¬ì ì§€ì†"ì— ì¶”ê°€ ìƒìŠ¹ ë‹¬ë ¸ë‹¤ [ê¹€í˜„ì„ì˜ ì›”ìŠ¤íŠ¸ë¦¬íŠ¸ë‚˜ìš°] - í•œêµ­ê²½ì œ

    2. íŠ¸ëŸ¼í”„ë°œ AI íˆ¬ì, í•œêµ­ê¸°ì—…ì— ê¸°íšŒ?â€¦ "ì „ë ¥ ì„¤ë£¨ì…˜ ì‹œì¥ ì£¼ëª©" - ì—°í•©ë‰´ìŠ¤

    3. ETFÂ·AI íˆ¬ìì˜ ìƒˆ ë¬¼ê²°â€¦ ê¸ˆë¦¬ í•˜ë½ê¸°ì˜ ì „ëµì  ìì‚° ë°°ë¶„ ê°€ì´ë“œ [AI PRISM*ì£¼ê°„ ê¸ˆìœµìƒí’ˆ íˆ¬ìì ë‰´ìŠ¤] - ì„œìš¸ê²½ì œì‹ ë¬¸

    4. AI íˆ¬ì ì ê²Œ í–ˆë‹¤ëŠ” ì´ìœ ë¡œ ì• í”Œì€ 3% ê¸‰ë“±, ì‹œì´ 1ìœ„ íƒˆí™˜ - ë‰´ìŠ¤1

    5. ì–´ë‹ì„œí”„ë¼ì´ì¦ˆ ë©”íƒ€ 4% ê¸‰ë“±.. â€œAIì¸í”„ë¼ íˆ¬ì ê³„ì†â€ - ë§¤ì¼ê²½ì œ

    # ì°¸ê³  ë§í¬:
    1. [MS, ë©”íƒ€ "AI íˆ¬ì ì§€ì†"ì— ì¶”ê°€ ìƒìŠ¹ ë‹¬ë ¸ë‹¤](https://news.google.com/rss/articles/CBMiWkFVX3lxTE8yeEdfOWxJSmtHSUJYS0ZXVDJaU1RWUTNzU01EVkJQaFpOREZNRE95REpjWU9YR3ZJTlAyODJVdzU2djNZSW5kUjRpVkdid3lfYU9USlNlLTRuZ9IBVEFVX3lxTE1xVzJZZ1lRQ3R6UnlwYkkwS1dsSVBWV2o5TmZoTjBDMGtRanZzMENMMlE2bE9PbHpGMjVsQ0ZOYlk1Wk96bWJGMzlkQ3pqVVZvTndmcw?oc=5)
    2. [íŠ¸ëŸ¼í”„ë°œ AI íˆ¬ì, í•œêµ­ê¸°ì—…ì— ê¸°íšŒ?â€¦ "ì „ë ¥ ì„¤ë£¨ì…˜ ì‹œì¥ ì£¼ëª©"](https://news.google.com/rss/articles/CBMiW0FVX3lxTE4zbUlUMW5PZ2xBWVBXQjRrdWM1ZlRlN3BoWE1VRkd3cHRlcTVKSEEtLUZrMWlKc2NkRFY5dmJ3YmlmUTctd2Q4SzNxQnJBZnJMYzJxOVo1WjB1em_SAV5BVV95cUxQWjVzN2t0ejY2akhjZGVGbnpRRWw3ZWUwckFLc3U3NXkwaFcxaTVCRWJHY0M0STNhMU1heURwWmlXa285YnJqa2h5NFdkamVEejhwQnpVNE9uVS1aWk9B?oc=5)
    3. [ETFÂ·AI íˆ¬ìì˜ ìƒˆ ë¬¼ê²°â€¦ ê¸ˆë¦¬ í•˜ë½ê¸°ì˜ ì „ëµì  ìì‚° ë°°ë¶„ ê°€ì´ë“œ](https://news.google.com/rss/articles/CBMiVkFVX3lxTFA4WnRkSE1kZnUteWhMdmJWSWFEWUs3Y0tTTVFjc0M2YmNNOEM5b0RHa3pDN0dneXp6ZG9tcGNiQjMtYXF0X2l6a3VoMWlrTHdSVDgyLXlR?oc=5)
    4. [AI íˆ¬ì ì ê²Œ í–ˆë‹¤ëŠ” ì´ìœ ë¡œ ì• í”Œì€ 3% ê¸‰ë“±, ì‹œì´ 1ìœ„ íƒˆí™˜](https://news.google.com/rss/articles/CBMiWEFVX3lxTE5IYk80R1pzV084dlpXODE3RkFOc2pVNzdDMkxFUzFGeE51QUZrb2tMMVo5cUptTExXS01jZEFlTXBQUWlTRXBmYUhfUjNXc24xU1daQkE?oc=5)
    5. [ì–´ë‹ì„œí”„ë¼ì´ì¦ˆ ë©”íƒ€ 4% ê¸‰ë“±.. â€œAIì¸í”„ë¼ íˆ¬ì ê³„ì†â€](https://news.google.com/rss/articles/CBMiUkFVX3lxTE5kVkZWUWxKZXRuUzQxRkFOc2pVNzdDMkxFUzFGeE51QUZrb2tMMVo5cUptTExXS01jZEFlTXBQUWlTRXBmYUhfUjNXc24xU1daQkE?oc=5)

    # #AI #íˆ¬ì #ì£¼ì‹ #ì¦ê¶Œ #ê²½ì œ #ì‹œí™© #ë§ˆì¼“ë‰´ìŠ¤

    # (ì´ ê²Œì‹œê¸€ì€ ì°¸ê³ ìš©ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìœ¼ë©°, ì‹¤ì œ íˆ¬ìëŠ” ì£¼ì˜ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì „ë¬¸ì ì¸ ìë¬¸ì„ ë°›ìœ¼ì‹œê¸° ë°”ëë‹ˆë‹¤.)
